
import { useState, useCallback } from 'react';

interface DocumentChunk {
  id: string;
  content: string;
  metadata: {
    fileName: string;
    chunkIndex: number;
  };
}

export const useRAG = () => {
  const [documents, setDocuments] = useState<DocumentChunk[]>([]);
  const [isProcessing, setIsProcessing] = useState(false);

  const processFiles = useCallback(async (files: File[]) => {
    setIsProcessing(true);
    const newDocuments: DocumentChunk[] = [];

    for (const file of files) {
      try {
        let content = '';
        
        if (file.type === 'application/pdf') {
          // For now, we'll simulate PDF processing
          // In a real implementation, you'd use pdf-parse or similar
          content = `[PDF Content from ${file.name}] - This would contain the actual extracted text from the PDF file.`;
        } else if (file.type === 'text/plain' || file.type === 'text/markdown') {
          content = await file.text();
        }

        // Simple chunking strategy - split by paragraphs or every 500 characters
        const chunks = chunkText(content, 500);
        
        chunks.forEach((chunk, index) => {
          newDocuments.push({
            id: `${file.name}-${index}`,
            content: chunk,
            metadata: {
              fileName: file.name,
              chunkIndex: index,
            },
          });
        });
      } catch (error) {
        console.error(`Error processing file ${file.name}:`, error);
      }
    }

    setDocuments(prev => [...prev, ...newDocuments]);
    setIsProcessing(false);
  }, []);

  const chunkText = (text: string, maxLength: number): string[] => {
    const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);
    const chunks: string[] = [];
    let currentChunk = '';

    for (const sentence of sentences) {
      if (currentChunk.length + sentence.length > maxLength && currentChunk.length > 0) {
        chunks.push(currentChunk.trim());
        currentChunk = sentence;
      } else {
        currentChunk += (currentChunk ? '. ' : '') + sentence;
      }
    }

    if (currentChunk.trim()) {
      chunks.push(currentChunk.trim());
    }

    return chunks;
  };

  const searchRelevantChunks = useCallback((query: string, limit: number = 3): DocumentChunk[] => {
    // Simple keyword-based search
    const queryWords = query.toLowerCase().split(/\s+/);
    
    const scoredChunks = documents.map(chunk => {
      const chunkWords = chunk.content.toLowerCase().split(/\s+/);
      const score = queryWords.reduce((acc, word) => {
        return acc + chunkWords.filter(cWord => cWord.includes(word)).length;
      }, 0);
      
      return { chunk, score };
    });

    return scoredChunks
      .filter(item => item.score > 0)
      .sort((a, b) => b.score - a.score)
      .slice(0, limit)
      .map(item => item.chunk);
  }, [documents]);

  const generateResponse = useCallback(async (query: string): Promise<string> => {
    const relevantChunks = searchRelevantChunks(query);
    
    if (relevantChunks.length === 0) {
      return "I don't have any relevant information in my knowledge base to answer your question. Please upload some documents first or ask about something else.";
    }

    // Simulate AI response generation
    const context = relevantChunks.map(chunk => chunk.content).join('\n\n');
    
    // This is a simple mock response - in a real implementation, 
    // you'd use an actual LLM API like OpenAI, Anthropic, or local models
    return `Based on the documents you've uploaded, here's what I found:

${context.substring(0, 300)}...

This information comes from: ${relevantChunks.map(c => c.metadata.fileName).join(', ')}

Note: This is a demo response. In a production system, this would be generated by an actual AI model that processes the context and provides a more natural response.`;
  }, [searchRelevantChunks]);

  return {
    documents,
    processFiles,
    generateResponse,
    isProcessing,
    documentCount: documents.length,
  };
};
