
import { useState, useCallback } from 'react';
import { adminDocuments } from '@/data/adminDocuments';

export interface DocumentChunk {
  id: string;
  content: string;
  metadata: {
    fileName: string;
    chunkIndex: number;
  };
}

export const useRAG = () => {
  const [documents] = useState<DocumentChunk[]>(adminDocuments);


  const searchRelevantChunks = useCallback((query: string, limit: number = 3): DocumentChunk[] => {
    // Simple keyword-based search
    const queryWords = query.toLowerCase().split(/\s+/);
    
    const scoredChunks = documents.map(chunk => {
      const chunkWords = chunk.content.toLowerCase().split(/\s+/);
      const score = queryWords.reduce((acc, word) => {
        return acc + chunkWords.filter(cWord => cWord.includes(word)).length;
      }, 0);
      
      return { chunk, score };
    });

    return scoredChunks
      .filter(item => item.score > 0)
      .sort((a, b) => b.score - a.score)
      .slice(0, limit)
      .map(item => item.chunk);
  }, [documents]);

  const generateResponse = useCallback(async (query: string): Promise<string> => {
    const relevantChunks = searchRelevantChunks(query);
    
    if (relevantChunks.length === 0) {
      return "I don't have any relevant information in my knowledge base to answer your question.";
    }

    // Simulate AI response generation
    const context = relevantChunks.map(chunk => chunk.content).join('\n\n');
    
    // This is a simple mock response - in a real implementation, 
    // you'd use an actual LLM API like OpenAI, Anthropic, or local models
    return `Based on the administrator-provided documents, here's what I found:

${context.substring(0, 300)}...

This information comes from: ${relevantChunks.map(c => c.metadata.fileName).join(', ')}

Note: This is a demo response. In a production system, this would be generated by an actual AI model that processes the context and provides a more natural response.`;
  }, [searchRelevantChunks]);

  return {
    documents,
    generateResponse,
    documentCount: documents.length,
  };
};
